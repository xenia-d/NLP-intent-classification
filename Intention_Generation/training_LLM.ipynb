{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c1460",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WildGuard_Dataset/our_annotations.json', 'r', encoding='utf-8') as f:\n",
    "    data_json = json.load(f)\n",
    "dataset = Dataset.from_list(data_json)\n",
    "dataset = dataset.remove_columns(['file_upload', 'drafts', 'predictions', 'meta', 'created_at', 'updated_at', 'inner_id', 'cancelled_annotations', 'total_predictions', 'comment_count', 'unresolved_comment_count', 'last_comment_updated_at', 'project', 'updated_by', 'comment_authors'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "total_annotations_expanded = 0\n",
    "expanded_data = []\n",
    "\n",
    "for item in dataset:\n",
    "    if len(item['annotations']) > 1:\n",
    "        counter += 1\n",
    "        total_annotations_expanded += len(item['annotations'])\n",
    "        # replace annotations with a random one of the annotations\n",
    "        for annotation in item['annotations']:\n",
    "            new_item = item.copy()\n",
    "            new_item['annotations'] = [annotation]\n",
    "            expanded_data.append(new_item)\n",
    "    else:\n",
    "        expanded_data.append(item)\n",
    "\n",
    "print(f\"Expanded {counter} items with multiple annotations.\")\n",
    "print(f\"Total annotations expanded: {total_annotations_expanded}\")\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "print(f\"New dataset size: {len(expanded_data)}\")\n",
    "\n",
    "# Create a new dataset with the modified data\n",
    "new_dataset = Dataset.from_list(expanded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6fb41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "modified_data = []\n",
    "\n",
    "for item in dataset:\n",
    "    if len(item['annotations']) > 1:\n",
    "        counter += 1\n",
    "        # replace annotations with a random one of the annotations\n",
    "        rand_index = random.randint(0, len(item['annotations']) - 1)\n",
    "        item['annotations'] = [item['annotations'][rand_index]]\n",
    "    modified_data.append(item)\n",
    "\n",
    "print(f'There are {counter} items with more than one annotation. A random annotation was chosen as the final annotation')\n",
    "\n",
    "# Create a new dataset with the modified data\n",
    "new_dataset = Dataset.from_list(modified_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b9fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "no_intent_label = 0\n",
    "\n",
    "reduced_data = []\n",
    "for item in new_dataset:\n",
    "    annotation = item['annotations'][0]['result']\n",
    "    if len(annotation) < 2 :\n",
    "        if annotation[0]['from_name'] == 'harmful_class':\n",
    "            # if there is only the harm label, we do not keep the item as it is missing intention\n",
    "            no_intent_label += 1\n",
    "            continue\n",
    "        elif annotation[0]['from_name'] != 'harmful_class':\n",
    "            # if there is only the intention label, we keep the item\n",
    "            reduced_data.append(item)\n",
    "    else:\n",
    "        remove_item = False\n",
    "        for res in annotation:\n",
    "            if res['from_name'] == 'harmful_class' and res['value']['choices'][0] == 'Flag for Removal':\n",
    "                remove_item = True\n",
    "                counter += 1\n",
    "        if not remove_item:\n",
    "            reduced_data.append(item)\n",
    "            \n",
    "print(\"flagged for removal:\", counter)\n",
    "print(\"no intent label:\", no_intent_label)\n",
    "print(f'Original dataset size: {len(new_dataset)}')\n",
    "print('Reduced dataset size:', len(reduced_data))\n",
    "reduced_data = Dataset.from_list(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607375c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfba14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  prepare the dataset for trainig by extracting the prompt in 'data', the intent, and the id\n",
    "final_data = []\n",
    "for item in reduced_data:\n",
    "    prompt = item['data']['prompt']\n",
    "    intent = item['annotations'][0]['result']\n",
    "    intent_label = None\n",
    "    for res in intent:\n",
    "        if res['from_name'] == 'intent':\n",
    "            intent_label = res['value']['text'][0]\n",
    "    final_data.append({\n",
    "        'id': item['id'],\n",
    "        'prompt': prompt,\n",
    "        'intent': intent_label\n",
    "    })\n",
    "final_dataset = Dataset.from_list(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot the distribution of lengths in the prompt and intent\n",
    "import matplotlib.pyplot as plt\n",
    "prompt_lengths = [len(item['prompt'].split()) for item in final_dataset]\n",
    "intent_lengths = [len(item['intent'].split()) for item in final_dataset]\n",
    "prompt_max = max(prompt_lengths)\n",
    "intent_max = max(intent_lengths)\n",
    "print(\"Maximum prompt length:\", [prompt_max])\n",
    "print(\"Maximum intent length:\", [intent_max])\n",
    "plt.hist(prompt_lengths, bins=30, alpha=0.5, label='Prompt Lengths')\n",
    "plt.hist(intent_lengths, bins=30, alpha=0.5, label='Intent Lengths')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn final_dataset into a pandas datagrame\n",
    "df = pd.DataFrame(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d126df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and model\n",
    "model_name = \"t5-small\"  # You can use other models like \"t5-base\" or \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"prompt\"]\n",
    "    targets = examples[\"intent\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=prompt_max, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=intent_max, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"id\"] = examples[\"id\"]  # Preserve the original id\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = final_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Split the dataset into train, validation, and test set\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "val_dataset = train_test_split[\"train\"].train_test_split(test_size=0.1)\n",
    "train_dataset = val_dataset[\"train\"]\n",
    "val_dataset = val_dataset[\"test\"]\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./train_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_test_loss\",\n",
    "    generation_max_length=intent_max\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the evaluation loss\n",
    "eval_results = trainer.evaluate(eval_dataset)\n",
    "print(f\"Evaluation Loss: {eval_results['eval_loss']}\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a078be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map predictions back to original ids\n",
    "for i, pred in enumerate(predictions.predictions):\n",
    "    original_id = eval_dataset[i][\"id\"]\n",
    "    pred = np.where(pred != -100, pred, tokenizer.pad_token_id)\n",
    "    print(f\"ID: {original_id}, Prediction: {tokenizer.decode(pred, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc61dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions directory if it does not exist\n",
    "if not os.path.exists('predictions'):\n",
    "    os.makedirs('predictions')\n",
    "\n",
    "# save the predictions to a json file\n",
    "with open('predictions/{}.json'.format(model_name), 'w', encoding='utf-8') as f:\n",
    "    for i, pred in enumerate(predictions.predictions):\n",
    "        original_id = eval_dataset[i][\"id\"]\n",
    "        pred = np.where(pred != -100, pred, tokenizer.pad_token_id)\n",
    "        decoded_pred = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "        true_intent = eval_dataset[i][\"intent\"]\n",
    "        json.dump({'id': original_id, 'prediction': decoded_pred, 'true_intent': true_intent}, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
