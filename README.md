# Intent Detection of Prompts by LLMs
Welcome to our repository investigating LLMs' ability to deduce the intent of a given prompt. Our project follows the order described below.

## 1. Investigating baseline harm classification and model uncertainty
In this section, found in the folder `Harm_Classification`, we investigate how well an LLM can classify a prompt as safe or harmful through Supervised Fine-Tuning. Then, we apply an Uncertainty Quantification (UQ) method to determine which prompts are most difficult to classify.

Note that while this is our implementation of UQ, the final outcome of this step (finding the most uncertain prompts) was provided by Group 8, who had their own implementation.



